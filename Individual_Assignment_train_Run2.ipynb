{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MickDobbsKildavin2/MachineLearning/blob/main/Individual_Assignment_train_Run2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32788936",
      "metadata": {
        "id": "32788936"
      },
      "source": [
        "## Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8484be82",
      "metadata": {
        "id": "8484be82"
      },
      "outputs": [],
      "source": [
        "import os                           # For file processing.\n",
        "import csv                          # For reading csv files.\n",
        "import shutil\n",
        "from keras.models import Model\n",
        "#from keras.optimizers import Adam\n",
        "from keras.optimizers import adam_v2\n",
        "\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "367598ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "367598ad",
        "outputId": "98f88657-4cfc-4d19-9d50-76d159bc5916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace Meta.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/gdrive')\n",
        "\n",
        "!unzip gdrive/My\\ Drive/data/archive.zip > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f1eebbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f1eebbb",
        "outputId": "6584e9a9-5da6-4f55-9e12-4cfcc082d843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_dir      =  /content\n",
            "working_dir      =  /content/data\n",
            "train_data_dir   =  /content/Train\n",
            "test_data_dir    =  /content/Test\n",
            "meta_data_dir    =  /content/Meta\n",
            "meta_csv_file    =  /content/Meta.csv\n"
          ]
        }
      ],
      "source": [
        "#Set directory values\n",
        "#split_size=0.1\n",
        "#print('split_size =',split_size)\n",
        "\n",
        "# Get the current directory.\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# the data will be written to a working directory.\n",
        "working_dir    = os.path.join(current_dir,'data')\n",
        "train_data_dir = os.path.join(current_dir,'Train')\n",
        "test_data_dir  = os.path.join(current_dir,'Test')\n",
        "meta_data_dir  = os.path.join(current_dir,'Meta')\n",
        "\n",
        "print('current_dir      = ',current_dir)\n",
        "print('working_dir      = ',working_dir)\n",
        "print('train_data_dir   = ',train_data_dir)\n",
        "print('test_data_dir    = ',test_data_dir)\n",
        "print('meta_data_dir    = ',meta_data_dir)\n",
        "meta_csv_file = (os.path.join(current_dir,'Meta.csv'))\n",
        "print('meta_csv_file    = ',meta_csv_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515ecea7",
      "metadata": {
        "id": "515ecea7"
      },
      "source": [
        "### To properly reference the 43 classes in the metadata, create a directory structure to allow the data generators to match the classes to the test, validation and training datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "35f98aa4",
      "metadata": {
        "id": "35f98aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5e0e52a8-a40c-4399-ee81-6ef57dd6c94a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4bbda8ed9ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print('to_move',to_move)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print('dest',dest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_move\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata config complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Meta/27.png'"
          ]
        }
      ],
      "source": [
        "# Open the meta.csv file and use the ClassId to build a directory structure that will allow the \n",
        "# build of a class subset to match the training and test data.\n",
        "with open(meta_csv_file,\"r\") as csvfile:\n",
        "    r= csv.reader(csvfile,delimiter =',')\n",
        "    # Loop through the rows\n",
        "    for i,row in enumerate(r):\n",
        "        if i == 0: \n",
        "            continue\n",
        "        # Extract the class label\n",
        "        label = row[1]\n",
        "        #print('label',label)\n",
        "        # Extract the image name\n",
        "        img_name = label + '.png'\n",
        "        #print('img_name',img_name)\n",
        "\n",
        "        # Creayte the sub-folder if it does not exist\n",
        "        dest = os.path.join(meta_data_dir,label)\n",
        "        if not os.path.isdir(dest):\n",
        "            os.makedirs(dest)\n",
        "        \n",
        "        # Copy files to the relevant working directory   \n",
        "        to_move = os.path.join(meta_data_dir,img_name)\n",
        "        #print('to_move',to_move)\n",
        "        #print('dest',dest)\n",
        "        shutil.copy(to_move,dest)\n",
        "\n",
        "print('metadata config complete.')        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a8c18ac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8c18ac7",
        "outputId": "783afb5b-030c-489a-eac6-684424d3f036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_subset    =  ['.~lock.ClassesInformation.ods#', '.~lock.ClassesInformationStrong.ods#', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '5', '6', '7']\n"
          ]
        }
      ],
      "source": [
        "class_subset = sorted(os.listdir(meta_data_dir))[:43] # Using only the first 10 classes\n",
        "print('class_subset    = ',class_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97a0be47",
      "metadata": {
        "id": "97a0be47"
      },
      "source": [
        "### Create a training data generator to resize the images to the VGG16 expected size and to split the training data into:\n",
        "  -  A training set\n",
        "  -  A validation set\n",
        "\n",
        "Use an 85% to 15% split for now.\n",
        "\n",
        "This code is based on the working example at : \n",
        "https://www.learndatasci.com/tutorials/convolutional-neural-networks-image-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3b878a17",
      "metadata": {
        "id": "3b878a17"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "# The default input size for this model is 224x224.\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "dim = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "\n",
        "train_generator = ImageDataGenerator(rotation_range=90, \n",
        "                                     brightness_range=[0.1, 0.7],\n",
        "                                     width_shift_range=0.5, \n",
        "                                     height_shift_range=0.5,\n",
        "                                     horizontal_flip=True, \n",
        "                                     vertical_flip=True,\n",
        "                                     validation_split=0.15,\n",
        "                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n",
        "\n",
        "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abeefb3c",
      "metadata": {
        "id": "abeefb3c"
      },
      "source": [
        "### Now use the data generator to create a training dataset.  This will resize the images to the VGG16 expected size and create a training dataset for the training model.  The resizing is to match the data that VGG16 was trained with.\n",
        "\n",
        "validation_split=0.15 so the test split will be 85%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "67bd5a76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67bd5a76",
        "outputId": "3f749bb1-276d-4e79-e551-3e42dacdb066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30888 images belonging to 43 classes.\n"
          ]
        }
      ],
      "source": [
        "traingen = train_generator.flow_from_directory(train_data_dir,\n",
        "                                               target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "                                               class_mode='categorical',\n",
        "                                               classes=class_subset,\n",
        "                                               subset='training',\n",
        "                                               batch_size=BATCH_SIZE, \n",
        "                                               shuffle=True,\n",
        "                                               seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96054492",
      "metadata": {
        "id": "96054492"
      },
      "source": [
        "### Create a validation data generator to resize the images to the VGG16 expected size and to assign some of the original training data to the validation set.\n",
        "\n",
        "validation_split=15%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c0b36d4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b36d4a",
        "outputId": "315459a6-b569-4705-9514-59580d06a268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5441 images belonging to 43 classes.\n"
          ]
        }
      ],
      "source": [
        "validgen = train_generator.flow_from_directory(train_data_dir,\n",
        "                                               target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "                                               class_mode='categorical',\n",
        "                                               classes=class_subset,\n",
        "                                               subset='validation',\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a similar way to the metadata used to set up the classification set, teh training data needs to be given a directory structure the same as the training data."
      ],
      "metadata": {
        "id": "Tq5UOOI4kdvq"
      },
      "id": "Tq5UOOI4kdvq"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "df8c852d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8c852d",
        "outputId": "8ee2da94-23b6-4006-ef00-44b8a8abddc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_csv_file    =  /content/Test.csv\n"
          ]
        }
      ],
      "source": [
        "### Create a test dataset.\n",
        "test_csv_file = (os.path.join(current_dir,'Test.csv'))\n",
        "print('test_csv_file    = ',test_csv_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e0a5a6a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0a5a6a7",
        "outputId": "6c6bf737-05f4-4cd1-83ca-7a7a4ca77fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data re-config complete.\n"
          ]
        }
      ],
      "source": [
        "# prepare_test(source_test_folder,test_csv_file)\n",
        "with open(test_csv_file,\"r\") as csvfile:\n",
        "    r= csv.reader(csvfile,delimiter =',')\n",
        "    for i,row in enumerate(r):\n",
        "        if i == 0: \n",
        "            continue\n",
        "        label = row[-2]\n",
        "        #print('label =',label)\n",
        "        img_name = row[-1]\n",
        "        #print('img_name =',img_name)\n",
        "        # Creayte the sub-folder if it does not exist\n",
        "        #print('test_data_dir =',test_data_dir)\n",
        "        dest = os.path.join(test_data_dir,label)\n",
        "        if not os.path.isdir(dest):\n",
        "            os.makedirs(dest)\n",
        "        #print('dest          =',dest)\n",
        "\n",
        "        # Copy files to the relevant working directory   \n",
        "        to_move = os.path.join(current_dir,img_name)\n",
        "        #print('to_move       =',to_move)\n",
        "        shutil.move(to_move,dest)\n",
        "print('test data re-config complete.')        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a validation data generator to resize the images to the VGG16 expected size and to assign some of the original training data to the validation set."
      ],
      "metadata": {
        "id": "zm1g5ejAlFUl"
      },
      "id": "zm1g5ejAlFUl"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "feec1c5e",
      "metadata": {
        "id": "feec1c5e",
        "outputId": "e25b9361-624a-4bb2-fba9-81dbc8068e76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11700 images belonging to 43 classes.\n"
          ]
        }
      ],
      "source": [
        "testgen = test_generator.flow_from_directory(test_data_dir,\n",
        "                                             target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "                                             class_mode=None,\n",
        "                                             classes=class_subset,\n",
        "                                             batch_size=1,\n",
        "                                             shuffle=False,\n",
        "                                             seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65b3ca4",
      "metadata": {
        "id": "e65b3ca4"
      },
      "source": [
        "Now that the data is set up to match the VGG16 model, we can create a new model based on VGG16 \n",
        "\n",
        "This is the key part of implementing the concept of Transfer Learning.\n",
        "\n",
        "This model will use pre-trained layers to process the GTSRB data, using a model built for image recognition to retrain a model specifically for the traffic sign data.\n",
        "\n",
        "This create_model proc is taken straight from https://www.learndatasci.com/tutorials/convolutional-neural-networks-image-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0249b305",
      "metadata": {
        "id": "0249b305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe658380-a310-4099-8c67-b982065aa0ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7efd522a9f50>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
        "    \"\"\"\n",
        "    Compiles a model integrated with VGG16 pretrained layers\n",
        "    \n",
        "    input_shape: tuple - the shape of input images (width, height, channels)\n",
        "    n_classes: int - number of classes for the output layer\n",
        "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
        "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
        "                If set to 0, all pretrained layers will freeze during training\n",
        "    \"\"\"\n",
        "    \n",
        "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
        "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
        "    conv_base = VGG16(include_top=False,\n",
        "                     weights='imagenet', \n",
        "                     input_shape=input_shape)\n",
        "    \n",
        "    # Defines how many layers to freeze during training.\n",
        "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
        "    # depending on the size of the fine-tuning parameter.\n",
        "    if fine_tune > 0:\n",
        "        for layer in conv_base.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
        "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
        "    top_model = conv_base.output\n",
        "    top_model = Flatten(name=\"flatten\")(top_model)\n",
        "    top_model = Dense(4096, activation='relu')(top_model)\n",
        "    top_model = Dense(1072, activation='relu')(top_model)\n",
        "    top_model = Dropout(0.2)(top_model)\n",
        "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
        "    \n",
        "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
        "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
        "\n",
        "    # Compiles the model for training.\n",
        "    model.compile(optimizer=optimizer, \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Set a learning rate and the shape for the input layer.    \n",
        "input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
        "optim_1 = adam_v2.Adam(learning_rate=0.001)\n",
        "n_classes=43\n",
        "\n",
        "# Set the values for the validation.    \n",
        "n_steps = traingen.samples // BATCH_SIZE\n",
        "n_val_steps = validgen.samples // BATCH_SIZE\n",
        "n_epochs = 15\n",
        "\n",
        "# First we'll train the model without Fine-tuning\n",
        "vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n",
        "vgg_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Having problems running this on colab and locally.  The only way it works for me is to do a dynamic install.\n",
        "!pip install livelossplot --quiet\n",
        "\n",
        "from livelossplot.inputs.keras import PlotLossesCallback\n",
        "plot_loss_1 = PlotLossesCallback()"
      ],
      "metadata": {
        "id": "6loXUhCnGTnN"
      },
      "id": "6loXUhCnGTnN",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set a checkpoint setting."
      ],
      "metadata": {
        "id": "LVrRs8NJnkMn"
      },
      "id": "LVrRs8NJnkMn"
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelCheckpoint callback - save best weights\n",
        "tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n",
        "                                  save_best_only=True,\n",
        "                                  verbose=1)"
      ],
      "metadata": {
        "id": "lw6-kKLxfBAY"
      },
      "id": "lw6-kKLxfBAY",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop training when the model has stopped improving.\n",
        "\n",
        "We want to minimize the loss so that is what is monitored for improvement here."
      ],
      "metadata": {
        "id": "Dq3iyppvnzQT"
      },
      "id": "Dq3iyppvnzQT"
    },
    {
      "cell_type": "code",
      "source": [
        "# EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='min')"
      ],
      "metadata": {
        "id": "Jie5h84nfBD6"
      },
      "id": "Jie5h84nfBD6",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#print('%%time',%%time)"
      ],
      "metadata": {
        "id": "qWzt36zHg_RI",
        "outputId": "e866cef7-32ad-4c1f-ad03-da99de71fc10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qWzt36zHg_RI",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 13.8 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "vgg_history = vgg_model.fit(traingen,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            #epochs=50,\n",
        "                            epochs=n_epochs,\n",
        "                            validation_data=validgen,\n",
        "                            steps_per_epoch=482,\n",
        "                            validation_steps=85,\n",
        "                            callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n",
        "                            verbose=1)"
      ],
      "metadata": {
        "id": "FRwyX90TfBJY",
        "outputId": "0c3a93c0-208a-4606-e6a8-28020f3d1329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FRwyX90TfBJY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset our image data generators\n",
        "traingen.reset()\n",
        "validgen.reset()\n",
        "testgen.reset()"
      ],
      "metadata": {
        "id": "nrncJ7ubfBNm"
      },
      "id": "nrncJ7ubfBNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f87gGAPMfBTf"
      },
      "id": "f87gGAPMfBTf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zegbBRQge1mU"
      },
      "id": "zegbBRQge1mU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from IPython.display import SVG, Image\n",
        "#from livelossplot import PlotLossesTensorFlowKeras\n",
        "from livelossplot import PlotLossesKeras\n",
        "from livelossplot.keras import PlotLossesCallback\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version:\", tf.__version__)"
      ],
      "metadata": {
        "id": "E5WajwDfKvA6",
        "outputId": "7030e03d-3034-4485-8add-d6eb1d1f6eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "id": "E5WajwDfKvA6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-37122f1ef145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install livelossplot --quiet"
      ],
      "metadata": {
        "id": "NQzKImdtczz3"
      },
      "id": "NQzKImdtczz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "#from livelossplot import PlotLossesCallback"
      ],
      "metadata": {
        "id": "iNH4l8Bic7OZ"
      },
      "id": "iNH4l8Bic7OZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_1 = PlotLosses()\n",
        "\n",
        "# ModelCheckpoint callback - save best weights\n",
        "tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n",
        "                                  save_best_only=True,\n",
        "                                  verbose=1)\n",
        "\n",
        "# EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='min')"
      ],
      "metadata": {
        "id": "TnMfXaLVdRI4",
        "outputId": "b9cc220b-97ab-476e-83f0-986b488149be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "id": "TnMfXaLVdRI4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f980543dda18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ModelCheckpoint callback - save best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   verbose=1)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Data_Pre_Processing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}